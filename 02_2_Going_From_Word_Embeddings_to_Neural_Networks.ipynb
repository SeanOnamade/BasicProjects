{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanOnamade/BasicProjects/blob/main/02_2_Going_From_Word_Embeddings_to_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825ffa0f-ab9e-4ec9-a7f7-a441e22bdd3a",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "825ffa0f-ab9e-4ec9-a7f7-a441e22bdd3a"
      },
      "source": [
        "# Going From Word Embeddings to Neural Networks\n",
        "Great job making it this far! In the previous notebook, we covered different types of representing concepts as discrete elements, from symbolic representation (a form of nondistributed representation) to distributed representation.\n",
        "\n",
        "This course section will now narrow down to focus on distributed representation and the basics of Artificial Neural Networks, but keep in mind that there are many different learning algorithms that use each type of representation. For example, a variety of very useful and common learning algorithms that use nondistributed representations include clustering methods and k-nearest neighbor algorithms, but this is out of scope for this notebook.\n",
        "\n",
        "The reason we are focusing on distributed representation and the basics of Artificial Neural Networks is because these are basic building blocks to understanding the inner workings of Large Language Models. To build up to that, we first need to step back and understand how we might represent language as numerical vectors, and how a learning algorithm can be built to learn from this experience.\n",
        "\n",
        "**Table of Contents:**\n",
        "1. [Word2vec](#word2vec)\n",
        "2. [Brief Intermission: Intro to Artificial Neural Networks](#ann)\n",
        "3. [Classifying Movie Review Sentiment](#sentiment)\n",
        "4. [Conclusion](#conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e1deef-36c2-41b0-abbb-5af7801024ac",
      "metadata": {
        "id": "d4e1deef-36c2-41b0-abbb-5af7801024ac"
      },
      "source": [
        "## <a name=\"word2vec\"> 1. Word2vec\n",
        "This section introduces a technique called Word2Vec to obtain vector representations of words. You will create your own word embeddings using Word2vec.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2d2485",
      "metadata": {
        "id": "7c2d2485"
      },
      "source": [
        "### Intro to Word2vec\n",
        "Word2vec is a technique used in natural language processing to process a large corpus of text and represent words as vectors, also referred to as word embeddings. It was developed in 2013 at Google. Once the word embeddings are created, there are a variety of uses for them, such as analyzing the similarity between corpuses, recommendation systems, and sentiment analysis of text.\n",
        "\n",
        "Note that Word2vec itself does not refer to one model or algorithm. It is a family of model architectures and optimizations. Within Word2vec, [the original paper](https://arxiv.org/pdf/1301.3781.pdf) proposed two different methods for learning representations of words, continuous bag-of-words and continuous skip-gram. We describe continuous skip-gram below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f783138c",
      "metadata": {
        "id": "f783138c"
      },
      "source": [
        "#### Continuous skip-gram\n",
        "\n",
        "A continuous skip-gram model predicts words within a certain range before and after the current word in a sentence. Take a look at the demonstration image below ([credits to Tensorflow](https://www.tensorflow.org/text/tutorials/word2vec)) for the eight-word long sentence `The wide road shimmered in the hot sun`:\n",
        "\n",
        "<div>\n",
        "<img src=\"resources/2.2/word2vec_skipgram.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Notice how the skip-grams consist of the `target_word`(highlighted in green) and all combinations of that `target_word` with each word in the context (highlighted in yellow). The context window size determines the span of words on either side of a `target_word` that are considered to be a `context_word` to form the list of skip-grams for each target word in the sentence. Note that the image above only shows three of the eight possible skip-grams lists formed for each window size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd95a3c3",
      "metadata": {
        "id": "fd95a3c3"
      },
      "source": [
        "### Let's create some skip-grams to be used for sentiment analysis!\n",
        "We will be using a dataset of 25k IMDB movie review [(Huggingface link)](https://huggingface.co/datasets/imdb)\n",
        "\n",
        "Execute the code cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6348c2e0",
      "metadata": {
        "id": "6348c2e0",
        "outputId": "1a1137d5-aca3-42d0-f14f-ebad972a19d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/cyang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "nltk.download('punkt')\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d9e518",
      "metadata": {
        "id": "d0d9e518"
      },
      "source": [
        "#### Load the dataset and use pandas to manually examine a few examples of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30936f9b",
      "metadata": {
        "id": "30936f9b",
        "outputId": "88227dcd-09a6-4b38-b809-54866d2188bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If only to avoid making this type of film in t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This film was probably inspired by Godard's Ma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
              "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
              "2  If only to avoid making this type of film in t...      0\n",
              "3  This film was probably inspired by Godard's Ma...      0\n",
              "4  Oh, brother...after hearing about this ridicul...      0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca313218",
      "metadata": {
        "id": "ca313218"
      },
      "source": [
        "#### Preprocessing text\n",
        "\n",
        "Text preprocessing occurs when training any model on text corpuses. However, the exact steps involved in preprocessing differ, depending on the goal of the model.\n",
        "\n",
        "In this case, we'd like to preprocess the text so that we remove information that doesn't provide any pertinent information to the positive/negative sentiment of the text. In this notebook, we will remove the HTML tags and lowercase all the words, since creating two embeddings for one word capitalized differently (e.g., `CURIOUS` and `Curious`) does not add any pertinent information for the model to classify the text's sentiment. We opt to not remove the punctuation, since our hypothesis is that the presence of punctuation such as `!` vs `...` can be useful in determining the sentiment of text.\n",
        "\n",
        "As another example, if we were trying to analyze the similarity between two website copies, we would not want to remove relevant information like capitalization or HTML tags, since those can contribute greatly to similarities/differences in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9265480",
      "metadata": {
        "id": "c9265480",
        "outputId": "a4127cf1-dda7-4c0b-e614-fec0ccbf3f7d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i rented i am curious-yellow from my video sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"i am curious: yellow\" is a risible and preten...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>if only to avoid making this type of film in t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this film was probably inspired by godard's ma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>oh, brother...after hearing about this ridicul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  i rented i am curious-yellow from my video sto...      0\n",
              "1  \"i am curious: yellow\" is a risible and preten...      0\n",
              "2  if only to avoid making this type of film in t...      0\n",
              "3  this film was probably inspired by godard's ma...      0\n",
              "4  oh, brother...after hearing about this ridicul...      0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Code credit to https://www.kaggle.com/code/abdmental01/text-preprocessing-nlp-steps-to-process-text/notebook\n",
        "\n",
        "# Lowercase all words\n",
        "df['text'] = df['text'].str.lower()\n",
        "# Remove any HTML tags\n",
        "def remove_html_tags(text):\n",
        "    pattern = re.compile('<.*?>')\n",
        "    return pattern.sub(r'', text)\n",
        "df['text'] = df['text'].apply(remove_html_tags)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ff2b9a",
      "metadata": {
        "id": "29ff2b9a"
      },
      "source": [
        "#### Split the dataset into a train and test dataset\n",
        "\n",
        "We'll use 70% of the data for training the Word2vec word embedding model (and the subsequent neural network for predicting the positive/negative sentiment of the review) and 30% of it for scoring the model's resulting accuracy.\n",
        "\n",
        "When creating these train/test splits, we need to shuffle the order of the dataset (by setting `shuffle_state=True`), since the first half of the dataset consists only of movie reviews with label = `0`. We want both labels represented in both the training and test datasets. This will become important later in the notebook when we train a neural network (don't worry about it for now!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8809c4fb",
      "metadata": {
        "id": "8809c4fb",
        "outputId": "dcfa6b78-564a-4fe6-9567-4b1794a2019e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value counts for Train sentiments\n",
            "1    8752\n",
            "0    8748\n",
            "Name: label, dtype: int64\n",
            "________________________________\n",
            "Value counts for Test sentiments\n",
            "0    3752\n",
            "1    3748\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def split_train_test(df, test_size=0.3, shuffle_state=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(df[['text']],\n",
        "                                                        df['label'],\n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size,\n",
        "                                                        random_state=42)\n",
        "    print(\"Value counts for Train sentiments\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"________________________________\")\n",
        "    print(\"Value counts for Test sentiments\")\n",
        "    print(Y_test.value_counts())\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911f8f42",
      "metadata": {
        "id": "911f8f42"
      },
      "source": [
        "#### Tokenize each movie review\n",
        "\n",
        "This is necessary for proper syntactical input into the Word2vec model we are using. Each review becomes a list of sentences, with each sentence becoming a list of words in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba0479e",
      "metadata": {
        "id": "bba0479e",
        "outputId": "8d040c97-c600-42a2-c09f-e908d37cd171",
        "colab": {
          "referenced_widgets": [
            "30545f5735634d969cdcba47a8d5d91d",
            "b4794fce9e93478180137df8676764dc"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30545f5735634d969cdcba47a8d5d91d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4794fce9e93478180137df8676764dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize_sentence(text: str) -> list:\n",
        "  sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
        "  return sentences\n",
        "\n",
        "X_train['tokenized_text'] = X_train['text'].progress_apply(tokenize_sentence)\n",
        "X_test['tokenized_text'] = X_test['text'].progress_apply(tokenize_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b83f716",
      "metadata": {
        "id": "5b83f716",
        "outputId": "d6546963-a81d-4a84-d168-d4a586544291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 188049 sentences in the training dataset\n"
          ]
        }
      ],
      "source": [
        "def create_nested_list_of_tokenized_sentences(df: pd.DataFrame) -> list:\n",
        "  df_exploded = df.explode('tokenized_text')\n",
        "  df_exploded.reset_index(drop=True, inplace=True)\n",
        "  sentences_text = list(df_exploded['tokenized_text'])\n",
        "  return sentences_text\n",
        "\n",
        "sentences_text = create_nested_list_of_tokenized_sentences(X_train)\n",
        "print(f\"There are {len(sentences_text)} sentences in the training dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b524e31",
      "metadata": {
        "id": "3b524e31"
      },
      "source": [
        "### Let's train a Word2vec model\n",
        "\n",
        "There are many parameters for Word2vec, which can be tuned, but we won't focus on parameter tuning in this notebook.\n",
        "\n",
        "To understand the parameters below though, you can glance at the definitions here:\n",
        "- `vector_size` denotes the length of the resulting vector that represents each token\n",
        "- `window` denotes the size of the window for continuous skip-gram\n",
        "- `min_count` denotes the minimum number of times a word has to occur in order to be included in the Word2vec representation\n",
        "- `sg` denotes whether we use skip-gram instead of bag of words (1 means true)\n",
        "- `seed` allows us to set a seed number to make this deterministic\n",
        "\n",
        "Execute the code cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc6a9d6",
      "metadata": {
        "id": "ccc6a9d6"
      },
      "outputs": [],
      "source": [
        "# Train the Word2vec model - this might take a bit to run\n",
        "model = Word2Vec(sentences=sentences_text, vector_size=100, window=5, min_count=5, sg=1, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fdc411c",
      "metadata": {
        "id": "8fdc411c"
      },
      "source": [
        "### What properties do we observe about these word embeddings?\n",
        "\n",
        "Let's find the top ten closest words in the embedding space to the words `enjoyable`, `boring`, and `loved` using cosine similarity (measured between 0 and 1 with 1 being an exact vector match).\n",
        "\n",
        "Execute the code cells below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd970a06",
      "metadata": {
        "id": "fd970a06",
        "outputId": "e0e803c9-7928-492d-8f30-80cee7a411d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('entertaining', 0.88079833984375),\n",
              " ('watchable', 0.8228954076766968),\n",
              " ('uplifting', 0.7767974734306335),\n",
              " ('accessible', 0.7718669772148132),\n",
              " ('undemanding', 0.7685814499855042),\n",
              " ('engrossing', 0.7664482593536377),\n",
              " ('heartwarming', 0.7610951662063599),\n",
              " ('addictive', 0.7598646283149719),\n",
              " ('rewarding', 0.756846010684967),\n",
              " ('satisfying', 0.7504492998123169)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Top ten closest words to \"enjoyable\" in the embedding space\n",
        "model.wv.most_similar(\"enjoyable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8f174b",
      "metadata": {
        "id": "bc8f174b",
        "outputId": "68c5d1e9-7743-48b3-9b8f-0c3a6b85bb04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('dull', 0.8673138618469238),\n",
              " ('tedious', 0.8511406183242798),\n",
              " ('pointless', 0.8353320956230164),\n",
              " ('unoriginal', 0.7825552821159363),\n",
              " ('uninteresting', 0.7795570492744446),\n",
              " ('preachy', 0.7795037031173706),\n",
              " ('confusing', 0.7696241140365601),\n",
              " ('predictable', 0.7688359022140503),\n",
              " ('unbelievable', 0.7660384178161621),\n",
              " ('illogical', 0.7642027735710144)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Top ten closest words to \"boring\" in the embedding space\n",
        "model.wv.most_similar(\"boring\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d068809f",
      "metadata": {
        "id": "d068809f",
        "outputId": "4f22b053-1392-4423-b482-2398967f9f69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('hated', 0.7692369818687439),\n",
              " ('liked', 0.748568594455719),\n",
              " ('enjoyed', 0.7188347578048706),\n",
              " ('adored', 0.7025707960128784),\n",
              " ('disliked', 0.6714362502098083),\n",
              " ('hate', 0.669241726398468),\n",
              " ('adore', 0.6677272319793701),\n",
              " ('listened', 0.6588900685310364),\n",
              " ('preferred', 0.6512401700019836),\n",
              " ('downloaded', 0.650779664516449)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Top ten closest words to \"loved\" in the embedding space\n",
        "model.wv.most_similar(\"loved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e9aaf16",
      "metadata": {
        "id": "5e9aaf16"
      },
      "source": [
        " Word embeddings encode the meaning of words such that words closer to each other in the vector space are expected to be used in similar contexts, and sometimes contain similar meanings.\n",
        "\n",
        " However, as you may observe in the example of `loved`, there is no guarantee that just because words occur in similar contexts that they are actually synonyms. In the case of movie reviews, it actually makes a lot of sense that the words `loved` and `hated` are used in similar ways (`I hated that movie`, `I loved that movie`). If we trained Word2Vec on Shakespearean literature instead, we would most likely see different results, as the context is very different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd81c19",
      "metadata": {
        "id": "ffd81c19",
        "outputId": "8164c2eb-9563-4efa-f8c9-8367f7f74972"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('silly', 0.8656347393989563),\n",
              " ('stupid', 0.8524393439292908),\n",
              " ('goofy', 0.8368464708328247),\n",
              " ('lame', 0.8202411532402039),\n",
              " ('cheezy', 0.801953136920929),\n",
              " ('unoriginal', 0.8005430102348328),\n",
              " ('daft', 0.7996158599853516),\n",
              " ('unrealistic', 0.7986085414886475),\n",
              " ('unbelievable', 0.7957474589347839),\n",
              " ('distasteful', 0.79521644115448)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Top ten closest words to \"dumb\" + \"corny\" in the embedding space\n",
        "model.wv.most_similar(['dumb', 'corny'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5aa605",
      "metadata": {
        "id": "fd5aa605",
        "outputId": "ee5426df-a161-4417-ef26-221cf29ce61a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('narration', 0.4795984923839569),\n",
              " ('editing', 0.47862666845321655),\n",
              " ('lighting', 0.4759976267814636),\n",
              " ('camera-work', 0.4463352560997009),\n",
              " ('cinematography', 0.44403335452079773),\n",
              " ('pacing', 0.4364607632160187),\n",
              " ('images', 0.4343623220920563),\n",
              " ('soundtrack', 0.4319990873336792),\n",
              " ('storytelling', 0.42912930250167847),\n",
              " ('photography', 0.4263083040714264)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Top ten closest words to \"corny\" - \"dumb\" in the embedding space\n",
        "model.wv.most_similar(positive=['corny'], negative=['dumb'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc894879",
      "metadata": {
        "id": "cc894879"
      },
      "source": [
        "You can also form loose analogies by adding and subtracting vectors in the embedding space. In the two examples above, you can see that summing the embeddings of the words `dumb` and `corny` results in other adjectives that seem related to the intersection of dumb and corny. If we take the embedding of `corny` and subtract `dumb` from it though, it results in a completely different set of results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bba18af",
      "metadata": {
        "id": "5bba18af"
      },
      "source": [
        "#### Conclusion to Word2vec and discussion on how bias in text influences embeddings\n",
        "\n",
        "Note that Word2vec is not the only way to generate word embeddings, and more recently, larger neural networks have been used to generate potentially richer and higher dimension representations of these words.\n",
        "\n",
        "Additionally, this training dataset only contained ~188k+ sentences (specifically in the domain of movie reviews), but imagine how a large amount of training data across multiple domains can allow us to gain a more general representation. However, this is where issues of biased and contaminated data arise. If there are many examples of bias and/or bigoted speech in a text dataset, then the embedding space may shift to accommodate for that bias in its representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e75f74",
      "metadata": {
        "id": "58e75f74",
        "outputId": "958c882f-9a03-463e-a30c-41a80284bd44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('gina', 0.664168119430542),\n",
              " ('boyfriend', 0.6634090542793274),\n",
              " ('prostitute', 0.6619176864624023),\n",
              " ('husband', 0.6538667678833008),\n",
              " ('maria', 0.6498344540596008),\n",
              " ('hermann', 0.6467203497886658),\n",
              " ('client', 0.6465845704078674),\n",
              " ('shin-ae', 0.6462011337280273),\n",
              " ('loretta', 0.6457286477088928),\n",
              " ('priest', 0.6451437473297119)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(positive=['doctor','woman'], negative=['man'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6345f0f2",
      "metadata": {
        "id": "6345f0f2",
        "outputId": "7914f13f-5c24-476d-b15f-f36f44a001f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('scientist', 0.6646406054496765),\n",
              " ('chief', 0.6150426268577576),\n",
              " ('mastermind', 0.6135583519935608),\n",
              " ('sheriff', 0.611149787902832),\n",
              " ('murderer', 0.6094347238540649),\n",
              " ('comrade', 0.6058278679847717),\n",
              " ('lawyer', 0.6004095673561096),\n",
              " ('goons', 0.5967718958854675),\n",
              " ('hit-man', 0.5965445041656494),\n",
              " ('cop', 0.5949344038963318)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(positive=['doctor','man'], negative=['woman'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d386b37a",
      "metadata": {
        "id": "d386b37a"
      },
      "source": [
        "In the examples of `doctor + woman - man` shown above, you can see that `prostitute` is one of the closest words to the resulting embedding, compared to `doctor + man - woman`, where words like `scientist` and `sheriff` are some of the closest words to the resulting embedding. One can infer that at least a few examples of movies reviewed in this dataset may have mentions of female doctors in similar contexts as prostitutes, and of male doctors in similar contexts as authoritative figures like scientists and sheriffs. This is biased by the contents of the movies in the dataset in this case. However, you can imagine how different societal biases may manifest in large datasets scraped from the internet. There is a lot of research surrounding how/whether larger datasets reduce different biases or amplify them.\n",
        "\n",
        "\n",
        "If you're interested, you can read more about these issues in word embeddings in Section 4.3 of [`On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? by Bender et al.`](https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf), published in 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d978f6",
      "metadata": {
        "id": "06d978f6"
      },
      "source": [
        "## <a name=\"ann\"> 2. Brief Intermission: Intro to Artificial Neural Networks\n",
        "\n",
        "Before we continue onwards to train a model to predict the sentiment of a movie review on the word embeddings we just created, we first need to take a quick step back to understand artificial neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee39edf4",
      "metadata": {
        "id": "ee39edf4"
      },
      "source": [
        "### What are neural networks?\n",
        "\n",
        "A neural network is a network of small computing units, each of which takes in a vector of input values and produces a single output value. There are many types of neural networks, such as feedforward networks, recurrent networks, convolutional networks, graph networks, etc. We will only focus on feedforward networks in the scope of this notebook.\n",
        "\n",
        "While a neural network can be trained for many different tasks, we will first focus on classification (predicting the correct label of a given input data). In this case, we are aiming to do binary classification: predicting whether a movie review is positive (1) or negative (0), given the movie review text itself.\n",
        "\n",
        "First, let's gain an intuition for the basic building blocks of a neural network. Let's start with a simple linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d81a636",
      "metadata": {
        "id": "9d81a636"
      },
      "source": [
        "#### Starting from Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3880882",
      "metadata": {
        "id": "f3880882"
      },
      "source": [
        "The simplest type of neural network we can build is a linear network, which is a network that predicts an output from an input based on a linear relationship.\n",
        "\n",
        "You may remember learning an equation `y = mx + b` (aka `F(x) = mx + b`) in a math class at some point. `y` represents the output number, given some input `x` and two parameters, `m` and `b`, which control the slope of the line and how high/low the line intersects with the y-axis. This is the gist of linear regression, as pictured below. We'd have to choose the values of `m` and `b` in order to find a best fit with the training data.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"resources/2.2/linear_regression_diagram.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "If we were to build a linear network that could learn to predict the sentiment of a movie review, based on its word embedding vector of length 100, it might look something like this. The `X` input is now a vector of length 100, and the network would learn the best fit values of `m` and `b` without us having to manually choose them, in a process called supervised learning.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"resources/2.2/linear_net_diagram.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "However, there may not be a linear relationship between a word embedding vector and its sentiment, which is why neural networks are much more powerful once we introduce non-linearity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6b4024",
      "metadata": {
        "id": "0a6b4024"
      },
      "source": [
        "#### Non-Linearity and Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6115e8",
      "metadata": {
        "id": "2b6115e8"
      },
      "source": [
        "Neural networks are so powerful because of their non-linear activation functions. Activation functions are nodes that take in a set of inputs and weights and return an output. These outputs can then be connected to other nodes in layers. When a network has more than two layers, that's when we start calling them deep neural networks - the word `deep` refers to the fact that there are multiple of these layers stacked together.\n",
        "\n",
        "The reason non-linear activation functions are so powerful is that it has been shown in proofs (check out the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)) that it is possible for a neural network to represent a very wide variety of relationships between inputs and outputs, as long as the appropriate weights are chosen/learned.\n",
        "\n",
        "In our case, because we have a fairly simple task (one feature and only two choices of output - positive or negative), we will stick with a 1-layer neural network, as illustrated below.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"resources/2.2/simple_nn_diagram.png\" width=\"800\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe66200f",
      "metadata": {
        "id": "fe66200f"
      },
      "source": [
        "## <a name=\"sentiment\"> 3. Classifying Movie Review Sentiment\n",
        "\n",
        "Let's use our Word2vec embeddings to build a shallow 1-layer network to classify a movie review as positive or negative!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cebd980b",
      "metadata": {
        "id": "cebd980b"
      },
      "source": [
        "### Build a 1-layer neural network to classify whether a IMDB movie review is positive or negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af617fc5",
      "metadata": {
        "id": "af617fc5",
        "outputId": "79237de2-8a5a-4230-ef53-ae05532c4926",
        "colab": {
          "referenced_widgets": [
            "437c724bbb754591864adf46c401e563",
            "8a8c267c07de42bdbdc4830063f21f65"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "437c724bbb754591864adf46c401e563",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a8c267c07de42bdbdc4830063f21f65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Let's average over the word embeddings in each review to get a single feature for each review\n",
        "\n",
        "def calculate_avg_review_embedding(tokenized_sentences: list) -> np.ndarray:\n",
        "  vectors = []\n",
        "  word2vec_model_vocab = list(model.wv.index_to_key)\n",
        "  for word_sentence in tokenized_sentences:\n",
        "    word_count = 0\n",
        "    single_sentence_embedding = np.zeros(100)\n",
        "    for word in word_sentence:\n",
        "      if word in word2vec_model_vocab:\n",
        "        single_sentence_embedding = single_sentence_embedding + model.wv[word]\n",
        "        word_count = word_count + 1\n",
        "    if word_count != 0:\n",
        "      single_sentence_embedding = single_sentence_embedding / word_count\n",
        "    vectors.append(single_sentence_embedding)\n",
        "  return sum(vectors) / len(vectors)\n",
        "\n",
        "X_train['avg_word_embedding'] = X_train['tokenized_text'].progress_apply(calculate_avg_review_embedding)\n",
        "X_test['avg_word_embedding'] = X_test['tokenized_text'].progress_apply(calculate_avg_review_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17052350",
      "metadata": {
        "id": "17052350",
        "outputId": "9aeeed45-0c67-4d42-8ad6-8c8ed35a0687"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=700, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(max_iter=700, random_state=42)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(max_iter=700, random_state=42)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate and train the neural network - this might take a minute to run\n",
        "clf = MLPClassifier(random_state=42, max_iter=700)\n",
        "clf.fit(np.stack(X_train['avg_word_embedding'].to_numpy()), Y_train['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e4ff1f",
      "metadata": {
        "id": "37e4ff1f"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810dec51",
      "metadata": {
        "id": "810dec51",
        "outputId": "e4ffbe8b-3ea5-4ee0-8762-e539be91a73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 84.10666666666667 %\n"
          ]
        }
      ],
      "source": [
        "score = clf.score(np.stack(X_test['avg_word_embedding'].to_numpy()), Y_test['label'])\n",
        "print(\"Accuracy:\", score*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b71de4d7",
      "metadata": {
        "id": "b71de4d7"
      },
      "source": [
        "We care about accuracy, but it's not the whole picture.\n",
        "\n",
        "Specifically, accuracy doesn't give us a sense of the rate of false positives (a movie review was negative, but the model classified it as positive) or false negatives (a movie review was positive, but the model classified it as negative). This is important to give us a better understanding of the potential pitfalls of a model during its evaluation.\n",
        "\n",
        "If you imagine that this was a binary classifier trained to detect whether a tumor was cancerous, we'd care a LOT about the false negatives (a tumor is indeed cancerous, but the model classified it as benign), even if the accuracy overall was really high.\n",
        "\n",
        "Let's inspect what the false positive / false negatives rates look like for this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800d6ad9",
      "metadata": {
        "id": "800d6ad9",
        "outputId": "58c8b75e-d555-44cd-ff7c-4207afca42b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of True negatives (model predicted negative, and review is actually negative):  3194\n",
            "# of False positives (model predicted positive, but review is actually negative):  558\n",
            "# of False negatives (model predicted negative, but review is actually positive):  634\n",
            "# of True positives (model predicted positive, and review is actually positive):  3114\n"
          ]
        }
      ],
      "source": [
        "Y_pred = clf.predict(np.stack(X_test['avg_word_embedding'].to_numpy()))\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test['label'], Y_pred).ravel()\n",
        "\n",
        "print(\"# of True negatives (model predicted negative, and review is actually negative): \", tn)\n",
        "print(\"# of False positives (model predicted positive, but review is actually negative): \", fp)\n",
        "print(\"# of False negatives (model predicted negative, but review is actually positive): \" , fn)\n",
        "print(\"# of True positives (model predicted positive, and review is actually positive): \", tp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e27f52",
      "metadata": {
        "id": "c6e27f52"
      },
      "source": [
        "#### How do word embeddings and neural networks relate to transformers (large language models)?\n",
        "\n",
        "A transformer is a deep learning architecture developed at Google in 2017. This architecture _specifically_ is what underpins the large language models released in the past 5 years. The unique point of a transformer is its multi-head attention mechanism, which allows the network to take in huge amounts of data and attend to different parts of the word sequences, allowing it to form connections between not just adjacent windows of words, but longer range dependencies and patterns across a large sequence of words. We will cover this more in detail in later sections.\n",
        "\n",
        "A big difference with transformers from what we've seen so far that we'd like to highlight is the difference in task objectives. For the simple neural network we just built, the task was to classify a movie review text as positive or negative (aka binary classification). In contrast, the typical task objective of a transformer consists of predicting the next token in a sequence of text, or \"filling in the blank\".\n",
        "\n",
        "A second big difference here is that with transformers, the creation of the word embedding happens within the neural network architecture in an \"embedding layer\". The overall intuition though is the same as what we did in this notebook – the neural networks first convert text into embeddings and then learn an objective over those embeddings.\n",
        "\n",
        "That's all we'll say about this topic for now, but hopefully this helps you get a sense for how word embeddings can be used to build models, whether they are used to classify the sentiment of text, or for predicting the next words in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4697a6",
      "metadata": {
        "id": "bd4697a6"
      },
      "source": [
        "## <a name=\"conclusion\"> Conclusion\n",
        "Now you've learned about how to go from representation to a neural network, specifically for learning language! The main things we hope you took away from this lesson is that word embeddings are used as a representation of words for a variety of tasks that take text as an input. Some examples of these tasks can include classifying the sentiment of text (e.g., a movie review, a product review), or even predicting the next word in a sentence.\n",
        "\n",
        "If you'd like to dive a bit deeper into the topics discussed in this notebook and solidify your understanding with great visualizations, we also highly recommend [this 30 minute video](https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=7) by Grant Sanderson, who runs the YouTube channel 3Blue1Brown.\n",
        "\n",
        "Next, you will learn about how to train a neural network using PyTorch, along with more of the core concepts in neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac60020e",
      "metadata": {
        "id": "ac60020e"
      },
      "source": [
        "#### Credits\n",
        "Many of the explanations and inspiration for exercises were taken from these sources:\n",
        "- [Tensorflow's tutorial on Word2vec](https://www.tensorflow.org/text/tutorials/word2vec)\n",
        "- [Word2vec Wikipedia](https://en.wikipedia.org/wiki/Word2vec)\n",
        "- [Speech and Language Processing Chapter 7](https://web.stanford.edu/~jurafsky/slp3/7.pdf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}